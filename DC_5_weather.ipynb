{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "su hduser\n",
    "\n",
    "    start-dfs.sh\n",
    "    start-yarn.sh\n",
    "\n",
    "    jps \n",
    "\n",
    "    ls \n",
    "\n",
    "\n",
    "hdfs dfs -ls / \n",
    "\n",
    "    hdfs dfs -rm -r /input\n",
    "\n",
    "    hdfs dfs -rm -r /output\n",
    "    \n",
    "    hdfs dfs -mkdir -p /input\n",
    "    \n",
    "    hdfs dfs -ls / \n",
    "    \n",
    "    nano weather.txt\n",
    "cat weather.txt\n",
    "    \n",
    "    hdfs dfs -put weather.txt /input/\n",
    "    \n",
    "    hdfs dfs -ls /input/\n",
    "    \n",
    "    nano mapper.py\n",
    "\n",
    "    nano reducer.py\n",
    "\n",
    "    chmod +x  mapper.py\n",
    "\n",
    "    chmod +x reducer.py\n",
    "\n",
    "\n",
    "    \n",
    "    whereis hadoop\n",
    "    \n",
    "    ls  /usr/local/hadoop/\n",
    "\n",
    "    ls  /usr/local/hadoop/share/ \n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/\n",
    "    \n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/\n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/lib/ \n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \n",
    "    \n",
    "    hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar -input /input/weather.txt -output /output/weather_output -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py \n",
    "\n",
    "    hdfs dfs -ls /\n",
    "    \n",
    "    hdfs dfs -ls /output/weather_output/\n",
    "    \n",
    "    hdfs dfs -cat /output/weather_output/part-00000\n",
    "    \n",
    "    stop-dfs.sh\n",
    "    stop-yarn.sh\n",
    "    \n",
    "*******************************Text***************************************\n",
    "\n",
    "Year,Month,Day,Max Temp (Â°C),Min Temp (Â°C),Rainfall (mm)\n",
    "1950,01,01,25,-18,43\n",
    "1950,01,02,26,-17,44\n",
    "1950,01,03,27,-12,32\n",
    "1950,01,04,28,-20,41\n",
    "1950,01,05,29,-13,40\n",
    "1950,01,06,30,-16,45\n",
    "1950,01,07,31,-14,33\n",
    "1950,01,08,32,-19,38\n",
    "1950,01,09,33,-20,28\n",
    "1950,01,10,34,-19,40\n",
    "\n",
    "******************************mapper***************************************\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Skip the header\n",
    "for idx, line in enumerate(sys.stdin):\n",
    "    if idx == 0:\n",
    "        continue  # Skip header\n",
    "    parts = line.strip().split(\",\")\n",
    "    if len(parts) != 6:\n",
    "        continue  # Skip malformed lines\n",
    "\n",
    "    year = parts[0]\n",
    "    try:\n",
    "        max_temp = float(parts[3])\n",
    "        min_temp = float(parts[4])\n",
    "    except ValueError:\n",
    "        continue  # Skip lines with non-numeric temperature\n",
    "\n",
    "    # Emit key-value pairs\n",
    "    print(f\"{year}\\t{max_temp},{min_temp},1\")\n",
    "    \n",
    "********************************reducer***********************************\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "temp_data = defaultdict(lambda: [0, 0, 0])  # max_sum, min_sum, count\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    year, values = parts\n",
    "    try:\n",
    "        max_temp, min_temp, count = map(float, values.split(\",\"))\n",
    "        temp_data[year][0] += max_temp\n",
    "        temp_data[year][1] += min_temp\n",
    "        temp_data[year][2] += count\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Output: Year -> avg max, avg min\n",
    "for year in sorted(temp_data):\n",
    "    max_sum, min_sum, count = temp_data[year]\n",
    "    avg_max = max_sum / count\n",
    "    avg_min = min_sum / count\n",
    "    print(f\"{year}\\tAvg Max Temp: {avg_max:.2f}, Avg Min Temp: {avg_min:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e653fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "**AIM:**\n",
    "To design and develop a distributed application to identify the coolest and hottest year from a given weather dataset using the MapReduce programming model.\n",
    "\n",
    "---\n",
    "\n",
    "**PROBLEM STATEMENT:**\n",
    "Design and develop a distributed application to find the coolest and hottest year from the available weather data. Use weather data from the internet and process it using MapReduce.\n",
    "\n",
    "---\n",
    "\n",
    "**THEORY:**\n",
    "\n",
    "**1. MapReduce:**\n",
    "MapReduce is a distributed computing model designed to process large datasets in parallel by dividing the work across multiple nodes in a computing cluster. It consists of two core functions:\n",
    "\n",
    "* **Map:** Processes input data and produces intermediate key-value pairs.\n",
    "* **Reduce:** Aggregates the intermediate data to generate the final output.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* Parallel processing across a cluster\n",
    "* Fault tolerance\n",
    "* Scalability for large datasets\n",
    "\n",
    "**2. Hadoop Streaming:**\n",
    "Hadoop Streaming is a utility that allows developers to create and run MapReduce jobs with any executable or script that uses standard input/output. This enables programming in languages like Python, Perl, Bash, or Ruby rather than Java. For this practical, Python is used due to its simplicity and clarity.\n",
    "\n",
    "---\n",
    "\n",
    "**WORKFLOW: Finding the Coolest and Hottest Year Using MapReduce**\n",
    "\n",
    "**Input File (HDFS):**\n",
    "\n",
    "* The weather dataset is uploaded to the Hadoop Distributed File System (HDFS).\n",
    "* Hadoop splits the file into blocks and distributes them across the cluster.\n",
    "\n",
    "**Step 1: Mapper Phase**\n",
    "\n",
    "* Reads each line from the input dataset.\n",
    "* Extracts relevant fields: Year, Max Temperature, Min Temperature.\n",
    "* Emits key-value pairs in the form:\n",
    "  `Year â†’ (max_temp, min_temp, count=1)`\n",
    "\n",
    "**Step 2: Shuffle and Sort** *(Handled by Hadoop)*\n",
    "\n",
    "* Groups data by key (year).\n",
    "* Sends all values for the same key to a single reducer.\n",
    "\n",
    "**Step 3: Reducer Phase**\n",
    "\n",
    "* Aggregates all temperature values for each year.\n",
    "* Calculates average, maximum, or minimum values as required.\n",
    "* Emits:\n",
    "  `Year â†’ (average_max_temp, average_min_temp)`\n",
    "\n",
    "**Step 4: Final Output**\n",
    "\n",
    "* The output is stored in HDFS.\n",
    "* You can determine the hottest and coolest year by checking the year with the highest average max temperature and the lowest average min temperature respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87012a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hereâ€™s a detailed explanation of the theory and **expected viva questions** with **their answers**, based on your problem statement and the dataset you've provided:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¬ **THEORY (Detailed for Viva)**\n",
    "\n",
    "**MapReduce Framework:**\n",
    "MapReduce is a framework by Google (implemented in Hadoop) used for processing massive amounts of data in a distributed environment. It breaks a job into two phases:\n",
    "\n",
    "* **Map**: Each Mapper processes a portion of the input data and emits intermediate key-value pairs.\n",
    "* **Reduce**: These intermediate pairs are then grouped by key, and each group is processed by a Reducer to generate final output.\n",
    "\n",
    "**Hadoop:**\n",
    "Hadoop is an open-source framework that uses HDFS (Hadoop Distributed File System) for storing large datasets and MapReduce for processing them.\n",
    "\n",
    "**Hadoop Streaming:**\n",
    "Hadoop Streaming allows using any programming language (like Python) to write Mapper and Reducer scripts. As long as the script reads from `stdin` and writes to `stdout`, it can be used with Hadoop.\n",
    "\n",
    "**Weather Dataset Processing Use-Case:**\n",
    "In this application, the dataset contains weather records with **date**, **maximum temperature**, and **minimum temperature**.\n",
    "Goal: Identify the **coolest year** and the **hottest year** based on temperature trends.\n",
    "\n",
    "**Map Phase:**\n",
    "\n",
    "* Each line is parsed.\n",
    "* Extract year, max temp, and min temp.\n",
    "* Emit key-value pair like:\n",
    "  `1950 â†’ 34,-20,1`\n",
    "\n",
    "**Reduce Phase:**\n",
    "\n",
    "* For each year:\n",
    "\n",
    "  * Total all max temps and min temps.\n",
    "  * Count entries.\n",
    "  * Compute average max and min temps.\n",
    "* Final result for each year:\n",
    "  `1950 â†’ Avg Max Temp, Avg Min Temp`\n",
    "\n",
    "From the final list, pick:\n",
    "\n",
    "* Year with **highest avg max temp â†’ Hottest year**\n",
    "* Year with **lowest avg min temp â†’ Coolest year**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ **Example from Your Dataset:**\n",
    "\n",
    "```\n",
    "Sample Line: 1950,01,10,34,-19,40  \n",
    "Extracted: Year = 1950, Max = 34, Min = -19  \n",
    "Emitted by Mapper: 1950 â†’ 34,-19,1\n",
    "```\n",
    "\n",
    "Reducer combines all such values for 1950 and computes the average.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¤ **VIVA QUESTIONS & ANSWERS**\n",
    "\n",
    "---\n",
    "\n",
    "**Q1. What is MapReduce and why is it used?**\n",
    "**A:** MapReduce is a programming model used for processing large-scale data in parallel. It simplifies distributed processing by splitting the job into two main tasksâ€”Map and Reduce. It's used because it efficiently handles huge datasets and supports scalability and fault-tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. What is the role of the Mapper and the Reducer?**\n",
    "**A:**\n",
    "\n",
    "* **Mapper** reads input line-by-line, processes it, and emits key-value pairs.\n",
    "* **Reducer** receives grouped data by key and performs aggregation or computation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. Why are we using Hadoop Streaming and not Java?**\n",
    "**A:** Hadoop Streaming allows the use of any language (like Python), not just Java. This is helpful for quick development and simplicity, especially for those who are more comfortable in scripting languages.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What key-value pair does the Mapper emit in your code?**\n",
    "**A:** For each valid line, the Mapper emits:\n",
    "`year â†’ max_temp, min_temp, count`\n",
    "E.g., `1950 â†’ 34,-19,1`\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. How does the Reducer process the values?**\n",
    "**A:**\n",
    "\n",
    "* Aggregates all values per year.\n",
    "* Sums max temps, min temps, and counts.\n",
    "* Calculates average temperatures using:\n",
    "  `avg_max = total_max / count`\n",
    "  `avg_min = total_min / count`\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How do you determine the coolest and hottest years?**\n",
    "**A:**\n",
    "\n",
    "* After Reducer output, parse all yearly results.\n",
    "* The year with **highest avg max temp** is the **hottest**.\n",
    "* The year with **lowest avg min temp** is the **coolest**.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. What challenges do you face while processing large datasets?**\n",
    "**A:**\n",
    "\n",
    "* Handling data distribution across nodes.\n",
    "* Fault tolerance and node failures.\n",
    "* Ensuring load balancing.\n",
    "* Memory and disk space limitations.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How is fault tolerance achieved in Hadoop?**\n",
    "**A:** Hadoop replicates data blocks across multiple nodes. If one node fails, another node's replica is used to continue the job without failure.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What format should the dataset be in for MapReduce?**\n",
    "**A:** Plain text with structured data, usually CSV or tab-separated. Each line should be independently processable.\n",
    "\n",
    "---\n",
    "\n",
    "**Q10. What will happen if the data has malformed lines or missing values?**\n",
    "**A:** Such lines are either skipped in the Mapper or handled with exception handling (try-except in Python), ensuring they donâ€™t break the processing flow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's a **line-by-line explanation with comments** for both the **Mapper** and **Reducer** code used in your MapReduce application to find the hottest and coolest years:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ¨ **Mapper Script: `mapper.py`**\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "```\n",
    "\n",
    "ðŸ’¬ This line specifies the script should be run using Python 3 â€” it helps in UNIX/Linux environments when running the script as an executable.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "```\n",
    "\n",
    "ðŸ’¬ Imports the `sys` module to read input line-by-line from standard input (`stdin`) â€” essential for Hadoop Streaming.\n",
    "\n",
    "```python\n",
    "# Skip the header\n",
    "for idx, line in enumerate(sys.stdin):\n",
    "```\n",
    "\n",
    "ðŸ’¬ Loops through each line from the input file. `enumerate()` provides both line index (`idx`) and the content.\n",
    "\n",
    "```python\n",
    "    if idx == 0:\n",
    "        continue  # Skip header\n",
    "```\n",
    "\n",
    "ðŸ’¬ Skips the first line, assuming it's the header of the CSV (e.g., \"Year,Month,Day,Max Temp,Min Temp,Rainfall\").\n",
    "\n",
    "```python\n",
    "    parts = line.strip().split(\",\")\n",
    "```\n",
    "\n",
    "ðŸ’¬ Removes any leading/trailing whitespace and splits the line by commas into a list.\n",
    "\n",
    "```python\n",
    "    if len(parts) != 6:\n",
    "        continue  # Skip malformed lines\n",
    "```\n",
    "\n",
    "ðŸ’¬ Ensures the line has exactly 6 values; if not, it's considered corrupt and skipped.\n",
    "\n",
    "```python\n",
    "    year = parts[0]\n",
    "```\n",
    "\n",
    "ðŸ’¬ Extracts the **year** from the first column (index 0).\n",
    "\n",
    "```python\n",
    "    try:\n",
    "        max_temp = float(parts[3])\n",
    "        min_temp = float(parts[4])\n",
    "```\n",
    "\n",
    "ðŸ’¬ Converts **Max Temp** and **Min Temp** (columns 3 and 4) to floating-point numbers.\n",
    "\n",
    "```python\n",
    "    except ValueError:\n",
    "        continue  # Skip lines with non-numeric temperature\n",
    "```\n",
    "\n",
    "ðŸ’¬ If conversion fails (e.g., missing or non-numeric data), that line is ignored.\n",
    "\n",
    "```python\n",
    "    # Emit key-value pairs\n",
    "    print(f\"{year}\\t{max_temp},{min_temp},1\")\n",
    "```\n",
    "\n",
    "ðŸ’¬ Emits the output in `key\\tvalue` format:\n",
    "\n",
    "* **Key** = Year\n",
    "* **Value** = max\\_temp, min\\_temp, count (1 occurrence for averaging later)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ© **Reducer Script: `reducer.py`**\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "```\n",
    "\n",
    "ðŸ’¬ Indicates to use Python 3 to run the script.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "```\n",
    "\n",
    "ðŸ’¬ Allows reading input from Hadoop Streamingâ€™s `stdin`.\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "```\n",
    "\n",
    "ðŸ’¬ Imports `defaultdict`, which simplifies initializing a dictionary with default values.\n",
    "\n",
    "```python\n",
    "temp_data = defaultdict(lambda: [0, 0, 0])  # max_sum, min_sum, count\n",
    "```\n",
    "\n",
    "ðŸ’¬ Creates a dictionary where:\n",
    "\n",
    "* `temp_data[year][0]`: Sum of all max temps\n",
    "* `temp_data[year][1]`: Sum of all min temps\n",
    "* `temp_data[year][2]`: Count of entries for that year\n",
    "\n",
    "```python\n",
    "for line in sys.stdin:\n",
    "```\n",
    "\n",
    "ðŸ’¬ Iterates through each line from the mapper's output.\n",
    "\n",
    "```python\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "```\n",
    "\n",
    "ðŸ’¬ Trims whitespace; skips empty lines.\n",
    "\n",
    "```python\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "```\n",
    "\n",
    "ðŸ’¬ Splits by tab character (`\\t`) to separate the key and value. Skips lines that don't have exactly two parts.\n",
    "\n",
    "```python\n",
    "    year, values = parts\n",
    "```\n",
    "\n",
    "ðŸ’¬ Extracts the year and the values (e.g., \"34,-20,1\").\n",
    "\n",
    "```python\n",
    "    try:\n",
    "        max_temp, min_temp, count = map(float, values.split(\",\"))\n",
    "```\n",
    "\n",
    "ðŸ’¬ Converts the comma-separated string into three float values.\n",
    "\n",
    "```python\n",
    "        temp_data[year][0] += max_temp\n",
    "        temp_data[year][1] += min_temp\n",
    "        temp_data[year][2] += count\n",
    "```\n",
    "\n",
    "ðŸ’¬ Updates the totals for each year.\n",
    "\n",
    "```python\n",
    "    except ValueError:\n",
    "        continue\n",
    "```\n",
    "\n",
    "ðŸ’¬ If parsing fails due to bad data, skips that line.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”½ **Output Phase**\n",
    "\n",
    "```python\n",
    "for year in sorted(temp_data):\n",
    "```\n",
    "\n",
    "ðŸ’¬ Loops through the years in sorted order.\n",
    "\n",
    "```python\n",
    "    max_sum, min_sum, count = temp_data[year]\n",
    "```\n",
    "\n",
    "ðŸ’¬ Retrieves the accumulated values for that year.\n",
    "\n",
    "```python\n",
    "    avg_max = max_sum / count\n",
    "    avg_min = min_sum / count\n",
    "```\n",
    "\n",
    "ðŸ’¬ Computes the **average max** and **average min** temperatures.\n",
    "\n",
    "```python\n",
    "    print(f\"{year}\\tAvg Max Temp: {avg_max:.2f}, Avg Min Temp: {avg_min:.2f}\")\n",
    "```\n",
    "\n",
    "ðŸ’¬ Outputs final results in a clean format with two decimal places.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE: \n",
    "mapper.py \n",
    "reducer.py \n",
    "1. Open Terminal and switch to Hadoop user \n",
    "pvg@pvg-HP-ProDesk-400-G4-SFF:~$ su hduser \n",
    "Password:  \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:/home/pvg$ cd \n",
    "2. Start HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-dfs.sh \n",
    "Starting namenodes on [localhost] \n",
    "Starting datanodes \n",
    "Starting secondary namenodes [pvg-HP-ProDesk-400-G4-SFF] \n",
    "2025-04-23 15:17:37,545 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-yarn.sh \n",
    "Starting resourcemanager \n",
    "Starting nodemanagers \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ jps \n",
    "16580 NodeManager \n",
    "16932 Jps \n",
    "15815 NameNode \n",
    "16216 SecondaryNameNode \n",
    "16425 ResourceManager \n",
    "15998 DataNode  \n",
    "3. Create an input directory \n",
    "#Delete any previous input or output files if present using: hdfs dfs -rm -r /input /output \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -mkdir -p /input \n",
    "2025-04-23 15:20:45,988 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls / \n",
    "2025-04-23 15:21:00,439 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 1 items \n",
    "drwxr-xr-x   - hduser supergroup          \n",
    "0 2025-04-23 15:20 /input \n",
    "4. Create a text file, paste the weather data and upload it to HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano weather_data.txt \n",
    "#weather_data.txt file will open in nano text editor \n",
    "#Copy the text from the provided weather data text file and paste it here \n",
    "#Press Ctrl + X \n",
    "#Press Y \n",
    "#Press Enter key \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -put weather_data.txt /input/ \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /input/ \n",
    "2025-04-23 15:21:32,321 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 1 items -rw-r--r--   1 hduser supergroup       \n",
    "/input/weather_data.txt \n",
    "1685 2025-04-23 15:21 \n",
    "5. Similary, create a mapper.py and reducer.py file \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano mapper.py \n",
    "#This will open the mapper.py file in nano editor; write the Mapper python \n",
    "code here \n",
    "#Hereâ€™s the code for mapper.py \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano reducer.py \n",
    "#Write the reducer python code \n",
    "#Now, make the python script executable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ chmod +x mapper.py \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ chmod +x reducer.py \n",
    "6. Run Hadoop streaming jar using the mapper and reducer scripts \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ whereis hadoop \n",
    "hadoop: /usr/local/hadoop /usr/local/hadoop/bin/hadoop.cmd \n",
    "/usr/local/hadoop/bin/Hadoop \n",
    "# - is used to specify flags \n",
    "# \\ at the end of a line is used to split a long command into multiple lines \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hadoop jar \n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\ \n",
    "> -input /input/weather_data.txt \\ \n",
    "> -output /output/weather_output \\ \n",
    "> -mapper mapper.py \\ \n",
    "> -reducer reducer.py \\ \n",
    "> -file mapper.py \\ \n",
    "> -file reducer.py \n",
    "2025-04-23 15:24:51,122 WARN streaming.StreamJob: -file option is deprecated, \n",
    "please use generic option -files instead. \n",
    "2025-04-23 15:24:51,217 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "packageJobJar: [mapper.py, reducer.py] [] /tmp/streamjob9268891282553717103.jar \n",
    "tmpDir=null \n",
    "2025-04-23 15:24:51,701 INFO impl.MetricsConfig: Loaded properties from hadoop\n",
    "metrics2.properties \n",
    "2025-04-23 15:24:51,769 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot \n",
    "period at 10 second(s). \n",
    "2025-04-23 15:24:51,769 INFO impl.MetricsSystemImpl: JobTracker metrics system \n",
    "started \n",
    "2025-04-23 15:24:51,784 WARN impl.MetricsSystemImpl: JobTracker metrics system \n",
    "already initialized! \n",
    "2025-04-23 15:24:51,993 INFO mapred.FileInputFormat: Total input files to \n",
    "process : 1 \n",
    "2025-04-23 15:24:52,053 INFO mapreduce.JobSubmitter: number of splits:1 \n",
    "2025-04-23 15:24:52,176 INFO mapreduce.JobSubmitter: Submitting tokens for job: \n",
    "job_local1388892213_0001 \n",
    "2025-04-23 15:24:52,176 INFO mapreduce.JobSubmitter: Executing with tokens: [] \n",
    "2025-04-23 15:24:52,326 INFO mapred.LocalDistributedCacheManager: Localized \n",
    "file:/home/hduser/mapper.py as \n",
    "file:/app/hadoop/tmp/mapred/local/job_local1388892213_0001_3cc73aa2-b9eb-4451\n",
    "bd53-20464abdb17f/mapper.py \n",
    "2025-04-23 15:24:52,360 INFO mapred.LocalDistributedCacheManager: Localized \n",
    "file:/home/hduser/reducer.py as \n",
    "file:/app/hadoop/tmp/mapred/local/job_local1388892213_0001_1494c792-fedb-4543\n",
    "be63-f5178e81a6a0/reducer.py \n",
    "2025-04-23 15:24:52,404 INFO mapreduce.Job: The url to track the job: \n",
    "http://localhost:8080/ \n",
    "2025-04-23 15:24:52,405 INFO mapred.LocalJobRunner: OutputCommitter set in config \n",
    "null \n",
    "2025-04-23 15:24:52,408 INFO mapreduce.Job: Running job: job_local1388892213_0001 \n",
    "2025-04-23 15:24:52,410 INFO mapred.LocalJobRunner: OutputCommitter is \n",
    "org.apache.hadoop.mapred.FileOutputCommitter \n",
    "2025-04-23 15:24:52,414 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:24:52,414 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:24:52,450 INFO mapred.LocalJobRunner: Waiting for map tasks \n",
    "2025-04-23 15:24:52,452 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local1388892213_0001_m_000000_0 \n",
    "2025-04-23 15:24:52,476 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:24:52,476 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:24:52,484 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-23 15:24:52,488 INFO mapred.MapTask: Processing split: \n",
    "hdfs://localhost:54310/input/weather_data.txt:0+1685 \n",
    "2025-04-23 15:24:52,503 INFO mapred.MapTask: numReduceTasks: 1 \n",
    "2025-04-23 15:24:52,536 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584) \n",
    "2025-04-23 15:24:52,536 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100 \n",
    "2025-04-23 15:24:52,536 INFO mapred.MapTask: soft limit at 83886080 \n",
    "2025-04-23 15:24:52,536 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600 \n",
    "2025-04-23 15:24:52,536 INFO mapred.MapTask: kvstart = 26214396; length = 6553600 \n",
    "2025-04-23 15:24:52,538 INFO mapred.MapTask: Map output collector class = \n",
    "org.apache.hadoop.mapred.MapTask$MapOutputBuffer \n",
    "2025-04-23 15:24:52,545 INFO streaming.PipeMapRed: PipeMapRed exec \n",
    "[/home/hduser/./mapper.py] \n",
    "2025-04-23 15:24:52,547 INFO Configuration.deprecation: mapred.work.output.dir is \n",
    "deprecated. Instead, use mapreduce.task.output.dir \n",
    "2025-04-23 15:24:52,548 INFO Configuration.deprecation: mapred.local.dir is \n",
    "deprecated. Instead, use mapreduce.cluster.local.dir \n",
    "2025-04-23 15:24:52,549 INFO Configuration.deprecation: map.input.file is \n",
    "deprecated. Instead, use mapreduce.map.input.file \n",
    "2025-04-23 15:24:52,549 INFO Configuration.deprecation: map.input.length is \n",
    "deprecated. Instead, use mapreduce.map.input.length \n",
    "2025-04-23 15:24:52,549 INFO Configuration.deprecation: mapred.job.id is \n",
    "deprecated. Instead, use mapreduce.job.id \n",
    "2025-04-23 15:24:52,550 INFO Configuration.deprecation: mapred.task.partition is \n",
    "deprecated. Instead, use mapreduce.task.partition \n",
    "2025-04-23 15:24:52,551 INFO Configuration.deprecation: map.input.start is \n",
    "deprecated. Instead, use mapreduce.map.input.start \n",
    "2025-04-23 15:24:52,551 INFO Configuration.deprecation: mapred.task.is.map is \n",
    "deprecated. Instead, use mapreduce.task.ismap \n",
    "2025-04-23 15:24:52,551 INFO Configuration.deprecation: mapred.task.id is \n",
    "deprecated. Instead, use mapreduce.task.attempt.id \n",
    "2025-04-23 15:24:52,552 INFO Configuration.deprecation: mapred.tip.id is \n",
    "deprecated. Instead, use mapreduce.task.id \n",
    "2025-04-23 15:24:52,552 INFO Configuration.deprecation: mapred.skip.on is \n",
    "deprecated. Instead, use mapreduce.job.skiprecords \n",
    "2025-04-23 15:24:52,553 INFO Configuration.deprecation: user.name is deprecated. \n",
    "Instead, use mapreduce.job.user.name \n",
    "2025-04-23 15:24:52,631 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,631 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,632 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,633 INFO streaming.PipeMapRed: Records R/W=100/1 \n",
    "2025-04-23 15:24:52,634 INFO streaming.PipeMapRed: MRErrorThread done \n",
    "2025-04-23 15:24:52,636 INFO streaming.PipeMapRed: mapRedFinished \n",
    "2025-04-23 15:24:52,638 INFO mapred.LocalJobRunner:  \n",
    "2025-04-23 15:24:52,638 INFO mapred.MapTask: Starting flush of map output \n",
    "2025-04-23 15:24:52,638 INFO mapred.MapTask: Spilling map output \n",
    "2025-04-23 15:24:52,638 INFO mapred.MapTask: bufstart = 0; bufend = 1185; bufvoid \n",
    "= 104857600 \n",
    "2025-04-23 15:24:52,638 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend \n",
    "= 26214000(104856000); length = 397/6553600 \n",
    "2025-04-23 15:24:52,648 INFO mapred.MapTask: Finished spill 0 \n",
    "2025-04-23 15:24:52,659 INFO mapred.Task: \n",
    "Task:attempt_local1388892213_0001_m_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-23 15:24:52,662 INFO mapred.LocalJobRunner: Records R/W=100/1 \n",
    "2025-04-23 15:24:52,662 INFO mapred.Task: Task \n",
    "'attempt_local1388892213_0001_m_000000_0' done. \n",
    "2025-04-23 15:24:52,666 INFO mapred.Task: Final Counters for \n",
    "attempt_local1388892213_0001_m_000000_0: Counters: 23 \n",
    "File System Counters \n",
    "FILE: Number of bytes read=1736 \n",
    "FILE: Number of bytes written=649844 \n",
    "FILE: Number of read operations=0 \n",
    "FILE: Number of large read operations=0 \n",
    "FILE: Number of write operations=0 \n",
    "HDFS: Number of bytes read=1685 \n",
    "HDFS: Number of bytes written=0 \n",
    "HDFS: Number of read operations=5 \n",
    "HDFS: Number of large read operations=0 \n",
    "HDFS: Number of write operations=1 \n",
    "HDFS: Number of bytes read erasure-coded=0 \n",
    "Map-Reduce Framework \n",
    "Map input records=100 \n",
    "Map output records=100 \n",
    "Map output bytes=1185 \n",
    "Map output materialized bytes=1391 \n",
    "Input split bytes=97 \n",
    "Combine input records=0 \n",
    "Spilled Records=100 \n",
    "Failed Shuffles=0 \n",
    "Merged Map outputs=0 \n",
    "GC time elapsed (ms)=6 \n",
    "Total committed heap usage (bytes)=196083712 \n",
    "File Input Format Counters  \n",
    "Bytes Read=1685 \n",
    "2025-04-23 15:24:52,667 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local1388892213_0001_m_000000_0 \n",
    "2025-04-23 15:24:52,667 INFO mapred.LocalJobRunner: map task executor complete. \n",
    "2025-04-23 15:24:52,669 INFO mapred.LocalJobRunner: Waiting for reduce tasks \n",
    "2025-04-23 15:24:52,669 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local1388892213_0001_r_000000_0 \n",
    "2025-04-23 15:24:52,675 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:24:52,675 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:24:52,675 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-23 15:24:52,677 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: \n",
    "org.apache.hadoop.mapreduce.task.reduce.Shuffle@77b2b11f \n",
    "2025-04-23 15:24:52,678 WARN impl.MetricsSystemImpl: JobTracker metrics system \n",
    "already initialized! \n",
    "2025-04-23 15:24:52,691 INFO reduce.MergeManagerImpl: MergerManager: \n",
    "memoryLimit=1437178240, maxSingleShuffleLimit=359294560, mergeThreshold=948537664, \n",
    "ioSortFactor=10, memToMemMergeOutputsThreshold=10 \n",
    "2025-04-23 15:24:52,692 INFO reduce.EventFetcher: \n",
    "attempt_local1388892213_0001_r_000000_0 Thread started: EventFetcher for fetching \n",
    "Map Completion Events \n",
    "2025-04-23 15:24:52,714 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle \n",
    "output of map attempt_local1388892213_0001_m_000000_0 decomp: 1387 len: 1391 to \n",
    "MEMORY \n",
    "2025-04-23 15:24:52,715 INFO reduce.InMemoryMapOutput: Read 1387 bytes from map\n",
    "output for attempt_local1388892213_0001_m_000000_0 \n",
    "2025-04-23 15:24:52,716 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map\n",
    "output of size: 1387, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, \n",
    "usedMemory ->1387 \n",
    "2025-04-23 15:24:52,717 INFO reduce.EventFetcher: EventFetcher is interrupted.. \n",
    "Returning \n",
    "2025-04-23 15:24:52,717 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:24:52,717 INFO reduce.MergeManagerImpl: finalMerge called with 1 \n",
    "in-memory map-outputs and 0 on-disk map-outputs \n",
    "2025-04-23 15:24:52,723 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-23 15:24:52,723 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 1373 bytes \n",
    "2025-04-23 15:24:52,726 INFO reduce.MergeManagerImpl: Merged 1 segments, 1387 \n",
    "bytes to disk to satisfy reduce memory limit \n",
    "2025-04-23 15:24:52,726 INFO reduce.MergeManagerImpl: Merging 1 files, 1391 bytes \n",
    "from disk \n",
    "2025-04-23 15:24:52,726 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes \n",
    "from memory into reduce \n",
    "2025-04-23 15:24:52,726 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-23 15:24:52,726 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 1373 bytes \n",
    "2025-04-23 15:24:52,727 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:24:52,729 INFO streaming.PipeMapRed: PipeMapRed exec \n",
    "[/home/hduser/./reducer.py] \n",
    "2025-04-23 15:24:52,731 INFO Configuration.deprecation: mapred.job.tracker is \n",
    "deprecated. Instead, use mapreduce.jobtracker.address \n",
    "2025-04-23 15:24:52,734 INFO Configuration.deprecation: mapred.map.tasks is \n",
    "deprecated. Instead, use mapreduce.job.maps \n",
    "2025-04-23 15:24:52,766 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,766 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,767 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:24:52,769 INFO streaming.PipeMapRed: Records R/W=100/1 \n",
    "2025-04-23 15:24:52,771 INFO streaming.PipeMapRed: MRErrorThread done \n",
    "2025-04-23 15:24:52,771 INFO streaming.PipeMapRed: mapRedFinished \n",
    "2025-04-23 15:24:52,809 INFO mapred.Task: \n",
    "Task:attempt_local1388892213_0001_r_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-23 15:24:52,812 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:24:52,812 INFO mapred.Task: Task \n",
    "attempt_local1388892213_0001_r_000000_0 is allowed to commit now \n",
    "2025-04-23 15:24:52,826 INFO output.FileOutputCommitter: Saved output of task \n",
    "'attempt_local1388892213_0001_r_000000_0' to \n",
    "hdfs://localhost:54310/output/weather_output \n",
    "2025-04-23 15:24:52,827 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce \n",
    "2025-04-23 15:24:52,827 INFO mapred.Task: Task \n",
    "'attempt_local1388892213_0001_r_000000_0' done. \n",
    "2025-04-23 15:24:52,827 INFO mapred.Task: Final Counters for \n",
    "attempt_local1388892213_0001_r_000000_0: Counters: 30 \n",
    "File System Counters \n",
    "FILE: Number of bytes read=4550 \n",
    "FILE: Number of bytes written=651235 \n",
    "FILE: Number of read operations=0 \n",
    "FILE: Number of large read operations=0 \n",
    "FILE: Number of write operations=0 \n",
    "HDFS: Number of bytes read=1685 \n",
    "HDFS: Number of bytes written=120 \n",
    "HDFS: Number of read operations=10 \n",
    "HDFS: Number of large read operations=0 \n",
    "HDFS: Number of write operations=3 \n",
    "HDFS: Number of bytes read erasure-coded=0 \n",
    "Map-Reduce Framework \n",
    "Combine input records=0 \n",
    "  Combine output records=0 \n",
    "  Reduce input groups=100 \n",
    "  Reduce shuffle bytes=1391 \n",
    "  Reduce input records=100 \n",
    "  Reduce output records=10 \n",
    "  Spilled Records=100 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=0 \n",
    "  Total committed heap usage (bytes)=196083712 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "  IO_ERROR=0 \n",
    "  WRONG_LENGTH=0 \n",
    "  WRONG_MAP=0 \n",
    "  WRONG_REDUCE=0 \n",
    " File Output Format Counters  \n",
    "  Bytes Written=120 \n",
    "2025-04-23 15:24:52,828 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local1388892213_0001_r_000000_0 \n",
    "2025-04-23 15:24:52,828 INFO mapred.LocalJobRunner: reduce task executor complete. \n",
    "2025-04-23 15:24:53,412 INFO mapreduce.Job: Job job_local1388892213_0001 running \n",
    "in uber mode : false \n",
    "2025-04-23 15:24:53,414 INFO mapreduce.Job:  map 100% reduce 100% \n",
    "2025-04-23 15:24:53,416 INFO mapreduce.Job: Job job_local1388892213_0001 \n",
    "completed successfully \n",
    "2025-04-23 15:24:53,435 INFO mapreduce.Job: Counters: 36 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=6286 \n",
    "  FILE: Number of bytes written=1301079 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=3370 \n",
    "  HDFS: Number of bytes written=120 \n",
    "  HDFS: Number of read operations=15 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=4 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Map input records=100 \n",
    "  Map output records=100 \n",
    "  Map output bytes=1185 \n",
    "  Map output materialized bytes=1391 \n",
    "  Input split bytes=97 \n",
    "  Combine input records=0 \n",
    "  Combine output records=0 \n",
    "  Reduce input groups=100 \n",
    "  Reduce shuffle bytes=1391 \n",
    "  Reduce input records=100 \n",
    "  Reduce output records=10 \n",
    "  Spilled Records=200 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=6 \n",
    "  Total committed heap usage (bytes)=392167424 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "  IO_ERROR=0 \n",
    "  WRONG_LENGTH=0 \n",
    "  WRONG_MAP=0 \n",
    "  WRONG_REDUCE=0 \n",
    " File Input Format Counters  \n",
    "  Bytes Read=1685 \n",
    " File Output Format Counters  \n",
    "  Bytes Written=120 \n",
    "2025-04-23 15:24:53,435 INFO streaming.StreamJob: Output directory: \n",
    "/output/weather_output \n",
    "7. View Output \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /output/weather_output/ \n",
    "2025-04-23 15:25:17,254 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 2 items -rw-r--r--   1 hduser supergroup          0 2025-04-23 15:24 \n",
    "/output/weather_output/_SUCCESS -rw-r--r--   1 hduser supergroup        120 2025-04-23 15:24 \n",
    "/output/weather_output/part-00000 \n",
    " \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -cat \n",
    "/output/weather_output/part-00000 \n",
    "2025-04-23 15:26:12,583 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "1950 -18 43 \n",
    "1951 -17 44 \n",
    "1952 -12 32 \n",
    "1953 -20 41 \n",
    "1954 -13 40 \n",
    "1955 -16 45 \n",
    "1956 -14 33 \n",
    "1957 -19 38 \n",
    "1958 -20 28 \n",
    "1959 -19 40 \n",
    " \n",
    "8. Stop HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-dfs.sh  \n",
    "Stopping namenodes on [localhost]  \n",
    "Stopping datanodes  \n",
    "Stopping secondary namenodes [Ubuntu]  \n",
    "2025-04-23 15:26:40,461 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable  \n",
    " \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-yarn.sh  \n",
    "Stopping nodemanagers  \n",
    "Stopping resourcemanager"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

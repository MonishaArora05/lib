{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "****************************Word Count*************************\n",
    "\n",
    "su hduser\n",
    "\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "\n",
    "jps \n",
    "\n",
    "ls \n",
    "nano word.txt\n",
    "cat word.txt\n",
    "\n",
    "hdfs dfs -ls / \n",
    "\n",
    "hdfs dfs -rm -r /input\n",
    "\n",
    "hdfs dfs -rm -r /output\n",
    "    \n",
    "hdfs dfs -mkdir -p /input\n",
    "    \n",
    "hdfs dfs -ls / \n",
    "    \n",
    "hdfs dfs -put word.txt /input/\n",
    "    \n",
    "hdfs dfs -ls /input/\n",
    "    \n",
    "whereis hadoop\n",
    "    \n",
    "ls  /usr/local/hadoop/\n",
    "\n",
    "ls  /usr/local/hadoop/share/ \n",
    "\n",
    "ls  /usr/local/hadoop/share/hadoop/\n",
    "    \n",
    "ls  /usr/local/hadoop/share/hadoop/mapreduce/\n",
    "    \n",
    "ls  /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \n",
    " \n",
    "hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount /input /output \n",
    "\n",
    "hdfs dfs -ls /\n",
    "    \n",
    "hdfs dfs -ls /output/\n",
    "    \n",
    "hdfs dfs -cat /output/part-r-00000\n",
    "    \n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63124d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "**su hduser**\n",
    "Switches to the Hadoop user account (usually required for running Hadoop commands).\n",
    "\n",
    "**start-dfs.sh**\n",
    "Starts the Hadoop Distributed File System (HDFS) daemons: NameNode, DataNode, etc.\n",
    "\n",
    "**start-yarn.sh**\n",
    "Starts the YARN daemons: ResourceManager and NodeManager.\n",
    "\n",
    "**jps**\n",
    "Displays the list of all Java processes currently running, useful to verify if Hadoop services started correctly.\n",
    "\n",
    "**ls**\n",
    "Lists files and directories in the current local (Linux) directory.\n",
    "\n",
    "**nano word.txt**\n",
    "Opens or creates a file named `word.txt` in the nano text editor for editing.\n",
    "\n",
    "**cat word.txt**\n",
    "Displays the contents of `word.txt` in the terminal.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists the contents of the root directory in HDFS.\n",
    "\n",
    "**hdfs dfs -rm -r /input**\n",
    "Deletes the `/input` directory from HDFS recursively, if it exists.\n",
    "\n",
    "**hdfs dfs -rm -r /output**\n",
    "Deletes the `/output` directory from HDFS recursively, if it exists.\n",
    "\n",
    "**hdfs dfs -mkdir -p /input**\n",
    "Creates the `/input` directory in HDFS. The `-p` ensures no error if the directory already exists.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists the contents of the root directory in HDFS again to confirm changes.\n",
    "\n",
    "**hdfs dfs -put word.txt /input/**\n",
    "Uploads the local file `word.txt` into the `/input` directory in HDFS.\n",
    "\n",
    "**hdfs dfs -ls /input/**\n",
    "Lists the contents of the `/input` directory in HDFS to verify the file upload.\n",
    "\n",
    "**whereis hadoop**\n",
    "Displays the path(s) where the `hadoop` command and related binaries are located.\n",
    "\n",
    "**ls /usr/local/hadoop/**\n",
    "Lists the contents of the local Hadoop installation directory.\n",
    "\n",
    "**ls /usr/local/hadoop/share/**\n",
    "Lists shared Hadoop components like common libraries and tools.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/**\n",
    "Shows Hadoop subcomponents like HDFS, MapReduce, YARN, etc.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/mapreduce/**\n",
    "Displays files and JARs specifically related to Hadoop MapReduce.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar**\n",
    "Verifies the presence of the example JAR file used to run demo MapReduce programs.\n",
    "\n",
    "**hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount /input /output**\n",
    "Executes the `wordcount` MapReduce example using `/input` as the input directory and stores results in `/output`.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists contents of the HDFS root again to confirm that the output directory has been created.\n",
    "\n",
    "**hdfs dfs -ls /output/**\n",
    "Displays the contents of the `/output` directory to view output files generated by MapReduce.\n",
    "\n",
    "**hdfs dfs -cat /output/part-r-00000**\n",
    "Displays the output content of the wordcount job (word frequencies) from the generated part file.\n",
    "\n",
    "**stop-dfs.sh**\n",
    "Stops all HDFS daemons like NameNode and DataNode.\n",
    "\n",
    "**stop-yarn.sh**\n",
    "Stops all YARN daemons like ResourceManager and NodeManager.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "*******************Character Count************************\n",
    "\n",
    "su hduser\n",
    "\n",
    "    start-dfs.sh\n",
    "    start-yarn.sh\n",
    "\n",
    "    jps \n",
    "\n",
    "    ls \n",
    "\n",
    "\n",
    "hdfs dfs -ls / \n",
    "\n",
    "    hdfs dfs -rm -r /input\n",
    "\n",
    "    hdfs dfs -rm -r /output\n",
    "    \n",
    "    hdfs dfs -mkdir -p /input\n",
    "    \n",
    "    hdfs dfs -ls / \n",
    "    \n",
    "    nano character.txt\n",
    "cat word.txt\n",
    "    \n",
    "    hdfs dfs -put character.txt /input/\n",
    "    \n",
    "    hdfs dfs -ls /input/\n",
    "    \n",
    "    nano mapper.py\n",
    "\n",
    "    nano reducer.py\n",
    "\n",
    "    chmod +x  mapper.py\n",
    "\n",
    "    chmod +x reducer.py\n",
    "\n",
    "\n",
    "    \n",
    "    whereis hadoop\n",
    "    \n",
    "    ls  /usr/local/hadoop/\n",
    "\n",
    "    ls  /usr/local/hadoop/share/ \n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/\n",
    "    \n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/\n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/lib/ \n",
    "\n",
    "    ls  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \n",
    "    \n",
    "    hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar -input /input/character.txt -output /output/character_output -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py \n",
    "\n",
    "    hdfs dfs -ls /\n",
    "    \n",
    "    hdfs dfs -ls /output/character_output/\n",
    "    \n",
    "    hdfs dfs -cat /output/character_output/part-00000\n",
    "    \n",
    "    stop-dfs.sh\n",
    "    stop-yarn.sh\n",
    "    \n",
    "********************************Mapper*********************************\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for char in line.strip():\n",
    "        print(f\"{char}\\t1\")\n",
    "        \n",
    "********************************Reducer********************************\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue  # skip empty lines\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 2:\n",
    "        continue  # skip malformed lines\n",
    "    key, val = parts\n",
    "    try:\n",
    "        counts[key] += int(val)\n",
    "    except ValueError:\n",
    "        continue  # skip lines with non-integer values\n",
    "\n",
    "for key in sorted(counts):\n",
    "    print(f\"{key}\\t{counts[key]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure! Here's your command list in the same **clean format with plain-text explanations** (not comments) — exactly like the second last format you asked for earlier. I’ve also flagged and corrected the minor issue you had in one of the commands:\n",
    "\n",
    "---\n",
    "\n",
    "**su hduser**\n",
    "Switches to the Hadoop user account, which has the permissions to run Hadoop services and jobs.\n",
    "\n",
    "**start-dfs.sh**\n",
    "Starts the Hadoop Distributed File System (HDFS) daemons such as NameNode and DataNode.\n",
    "\n",
    "**start-yarn.sh**\n",
    "Starts the YARN resource management daemons such as ResourceManager and NodeManager.\n",
    "\n",
    "**jps**\n",
    "Displays all running Java processes — used to confirm that Hadoop daemons started successfully.\n",
    "\n",
    "**ls**\n",
    "Lists all files and directories in the current local directory.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists the files and directories in the root directory of HDFS.\n",
    "\n",
    "**hdfs dfs -rm -r /input**\n",
    "Recursively deletes the `/input` directory in HDFS if it exists.\n",
    "\n",
    "**hdfs dfs -rm -r /output**\n",
    "Recursively deletes the `/output` directory in HDFS if it exists.\n",
    "\n",
    "**hdfs dfs -mkdir -p /input**\n",
    "Creates the `/input` directory in HDFS. The `-p` flag prevents an error if the path already exists.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists the contents of the root directory in HDFS again to confirm changes.\n",
    "\n",
    "**nano character.txt**\n",
    "Opens the nano editor to create or edit a file named `character.txt` in the local file system.\n",
    "\n",
    "**cat word.txt**\n",
    "Displays the content of the local file `word.txt`.\n",
    "\n",
    "**hdfs dfs -put character.txt /input/**\n",
    "Uploads the local `character.txt` file to the `/input` directory in HDFS.\n",
    "\n",
    "**hdfs dfs -ls /input/**\n",
    "Lists the contents of the `/input` directory in HDFS to verify that the file was uploaded.\n",
    "\n",
    "**nano mapper.py**\n",
    "Opens the nano editor to create or edit a Python script that will serve as the mapper.\n",
    "\n",
    "**nano reducer.py**\n",
    "Opens the nano editor to create or edit a Python script that will serve as the reducer.\n",
    "\n",
    "**chmod +x mapper.py**\n",
    "Grants execute permissions to the mapper script so it can be run during streaming.\n",
    "\n",
    "**chmod +x reducer.py**\n",
    "Grants execute permissions to the reducer script so it can be run during streaming.\n",
    "\n",
    "**whereis hadoop**\n",
    "Displays the locations where the Hadoop binary is installed and related files are found.\n",
    "\n",
    "**ls /usr/local/hadoop/**\n",
    "Lists the contents of the local Hadoop installation directory.\n",
    "\n",
    "**ls /usr/local/hadoop/share/**\n",
    "Lists the shared library directory inside the Hadoop installation.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/**\n",
    "Lists all core Hadoop components like HDFS, MapReduce, YARN, and tools.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/tools/**\n",
    "Lists all tools provided by Hadoop, including streaming and additional utilities.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/tools/lib/**\n",
    "Lists all JAR files under the tools library directory, including the streaming jar.\n",
    "\n",
    "**ls /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar **\n",
    "**hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar -input /input/character.txt -output /output/character_output -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py**\n",
    "Runs a Hadoop streaming job using the `mapper.py` and `reducer.py` scripts on the input file `/input/character.txt` and stores the results in `/output/character_output`.\n",
    "\n",
    "**hdfs dfs -ls /**\n",
    "Lists the contents of the root directory in HDFS to confirm the output was created.\n",
    "\n",
    "**hdfs dfs -ls /output/character_output/**\n",
    "Lists the output files generated by the MapReduce job under the `character_output` directory.\n",
    "\n",
    "**hdfs dfs -cat /output/character_output/part-00000**\n",
    "Displays the content of the final output file produced by the MapReduce streaming job.\n",
    "\n",
    "**stop-dfs.sh**\n",
    "Stops all HDFS daemons such as NameNode and DataNode.\n",
    "\n",
    "**stop-yarn.sh**\n",
    "Stops all YARN daemons such as ResourceManager and NodeManager.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**AIM:**\n",
    "To design a distributed application using MapReduce under Hadoop for:\n",
    "**a)** Character counting in a given text file.\n",
    "**b)** Counting the number of occurrences of every word in a given text file.\n",
    "\n",
    "---\n",
    "\n",
    "**PROBLEM STATEMENT:**\n",
    "Develop a distributed system using Hadoop MapReduce to process large text files and perform:\n",
    "\n",
    "* Word count: Count how many times each word appears.\n",
    "* Character count: Count how many times each character appears.\n",
    "\n",
    "---\n",
    "\n",
    "**THEORY:**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In the age of big data, organizations collect vast volumes of data from various sources like social media, sensors, and transaction logs. Traditional single-machine systems fall short in handling this scale due to hardware limitations. Apache Hadoop addresses these challenges by offering a scalable, fault-tolerant, and distributed computing solution.\n",
    "\n",
    "One of Hadoop’s core components is **MapReduce** — a programming paradigm that processes large datasets in parallel by dividing the task into:\n",
    "\n",
    "* **Map** Phase: Processes input data and outputs intermediate key-value pairs.\n",
    "* **Reduce** Phase: Aggregates and summarizes those intermediate results.\n",
    "\n",
    "The classic example is a **Word Count** problem, often referred to as the “Hello World” of MapReduce, because it clearly demonstrates distributed processing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hadoop Streaming**\n",
    "\n",
    "Hadoop Streaming allows developers to implement **Mapper** and **Reducer** functions in any language (Python, Bash, Perl, etc.) that supports standard I/O, offering flexibility and faster development, especially with Python’s simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "## **A) Word Count using MapReduce**\n",
    "\n",
    "### **1. Input File (HDFS)**\n",
    "\n",
    "* A plain text file is stored in the Hadoop Distributed File System (HDFS).\n",
    "* Hadoop splits the file into blocks and processes them in parallel.\n",
    "\n",
    "### **2. Mapper Phase**\n",
    "\n",
    "* Reads the file line by line.\n",
    "* Splits each line into words.\n",
    "* Emits a key-value pair: `(word, 1)`\n",
    "\n",
    "**Example Input:**\n",
    "\n",
    "```\n",
    "hello hadoop  \n",
    "hadoop program  \n",
    "hadoop world  \n",
    "hello world  \n",
    "```\n",
    "\n",
    "**Mapper Output:**\n",
    "\n",
    "```\n",
    "(\"hello\", 1)  \n",
    "(\"hadoop\", 1)  \n",
    "(\"hadoop\", 1)  \n",
    "(\"program\", 1)  \n",
    "(\"hadoop\", 1)  \n",
    "(\"world\", 1)  \n",
    "(\"hello\", 1)  \n",
    "(\"world\", 1)\n",
    "```\n",
    "\n",
    "### **3. Shuffle and Sort (Handled by Hadoop)**\n",
    "\n",
    "* Intermediate key-value pairs are grouped by key and passed to reducers:\n",
    "\n",
    "```\n",
    "(\"hadoop\", [1, 1, 1])  \n",
    "(\"hello\", [1, 1])  \n",
    "(\"program\", [1])  \n",
    "(\"world\", [1, 1])\n",
    "```\n",
    "\n",
    "### **4. Reducer Phase**\n",
    "\n",
    "* Each reducer sums the list of values per key:\n",
    "\n",
    "```\n",
    "(\"hadoop\", 3)  \n",
    "(\"hello\", 2)  \n",
    "(\"program\", 1)  \n",
    "(\"world\", 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **B) Character Count using MapReduce**\n",
    "\n",
    "### **1. Input File (HDFS)**\n",
    "\n",
    "* Same storage and parallel processing strategy as in word count.\n",
    "\n",
    "### **2. Mapper Phase**\n",
    "\n",
    "* Reads each line.\n",
    "* Splits the line into characters.\n",
    "* Emits a key-value pair: `(character, 1)`\n",
    "\n",
    "**Example Input:**\n",
    "\n",
    "```\n",
    "abbc  \n",
    "ccba  \n",
    "aacb\n",
    "```\n",
    "\n",
    "**Mapper Output:**\n",
    "\n",
    "```\n",
    "(\"a\", 1), (\"b\", 1), (\"b\", 1), (\"c\", 1)  \n",
    "(\"c\", 1), (\"c\", 1), (\"b\", 1), (\"a\", 1)  \n",
    "(\"a\", 1), (\"a\", 1), (\"c\", 1), (\"b\", 1)\n",
    "```\n",
    "\n",
    "### **3. Shuffle and Sort (Handled by Hadoop)**\n",
    "\n",
    "```\n",
    "(\"a\", [1, 1, 1, 1])  \n",
    "(\"b\", [1, 1, 1, 1])  \n",
    "(\"c\", [1, 1, 1])\n",
    "```\n",
    "\n",
    "### **4. Reducer Phase**\n",
    "\n",
    "* Each reducer aggregates character counts:\n",
    "\n",
    "```\n",
    "(\"a\", 4)  \n",
    "(\"b\", 4)  \n",
    "(\"c\", 3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "MapReduce is a powerful distributed framework to process large-scale text data. With minimal code and optimal resource use, both word and character frequency analyses are efficiently performed across a cluster using Python and Hadoop Streaming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24482a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🌐 THEORY: MapReduce using Hadoop (Word Count + Character Count)\n",
    "\n",
    "### 🧠 1. **What is Hadoop?**\n",
    "\n",
    "Hadoop is an open-source framework developed by Apache that allows processing of large data sets across clusters of computers using simple programming models. It is designed to scale from single servers to thousands of machines, offering:\n",
    "\n",
    "* Distributed storage via **HDFS (Hadoop Distributed File System)**\n",
    "* Distributed processing via **MapReduce**\n",
    "\n",
    "### ⚙️ 2. **What is MapReduce?**\n",
    "\n",
    "MapReduce is a **parallel and distributed programming model**. It allows large-scale data processing by breaking a task into two main phases:\n",
    "\n",
    "* **Map Phase:** Processes input and emits intermediate key-value pairs.\n",
    "* **Reduce Phase:** Collects and aggregates those intermediate outputs.\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "* *Map = “What do you want to count?”*\n",
    "* *Reduce = “Sum them up!”*\n",
    "\n",
    "### 💡 Example (Word Count):\n",
    "\n",
    "Say we have a file:\n",
    "\n",
    "```\n",
    "hello hadoop  \n",
    "hadoop world  \n",
    "```\n",
    "\n",
    "**Mapper Output:**\n",
    "\n",
    "```\n",
    "(\"hello\", 1), (\"hadoop\", 1), (\"hadoop\", 1), (\"world\", 1)\n",
    "```\n",
    "\n",
    "**Reducer Output:**\n",
    "\n",
    "```\n",
    "(\"hello\", 1), (\"hadoop\", 2), (\"world\", 1)\n",
    "```\n",
    "\n",
    "### 🧵 3. **What is Hadoop Streaming?**\n",
    "\n",
    "* Hadoop is Java-based, but Hadoop Streaming lets you use any language (like Python) to write Mapper and Reducer.\n",
    "* You just need to ensure your code reads from `stdin` and writes to `stdout`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 DIFFERENCE BETWEEN WORD COUNT & CHARACTER COUNT:\n",
    "\n",
    "| Feature       | Word Count                         | Character Count                       |\n",
    "| ------------- | ---------------------------------- | ------------------------------------- |\n",
    "| Input         | Line is split into words           | Line is split into characters         |\n",
    "| Mapper Output | (\"word\", 1)                        | (\"character\", 1)                      |\n",
    "| Purpose       | Count frequency of each word       | Count frequency of each character     |\n",
    "| Use Case      | Text analytics, keyword extraction | Spell correction, character-level NLP |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 EXPECTED VIVA QUESTIONS (and how to answer them like a pro):\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What is the role of the Mapper and Reducer in Hadoop?**\n",
    "\n",
    "* **Mapper:** Takes input line-by-line, processes each line, and emits intermediate key-value pairs.\n",
    "  👉 *In our case: (“word”, 1) or (“char”, 1)*\n",
    "* **Reducer:** Aggregates the intermediate values for each key and emits the final count.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is Hadoop Streaming and why did you use Python?**\n",
    "\n",
    "* Hadoop Streaming allows using non-Java languages like Python for writing MapReduce jobs.\n",
    "* We used Python because it’s easier to read, write, and debug for small-scale educational tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. How does the Shuffle and Sort phase work in MapReduce?**\n",
    "\n",
    "* It happens between the Map and Reduce phases.\n",
    "* Hadoop **automatically groups** all values with the same key and **sends them to the correct reducer**.\n",
    "* Think of it as the *glue* that connects mapper output to reducer input.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Can you explain how HDFS works?**\n",
    "\n",
    "* HDFS splits a file into fixed-size blocks (default 128 MB).\n",
    "* These blocks are stored across multiple nodes in the cluster.\n",
    "* It’s **fault-tolerant** by replicating each block (usually 3 copies).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Why is Word Count considered the “Hello World” of Hadoop?**\n",
    "\n",
    "* Because it is the simplest and most common use case to learn the working of MapReduce.\n",
    "* Helps students understand **parallel processing**, **key-value pairs**, and the **MapReduce flow**.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What happens if a node fails during execution?**\n",
    "\n",
    "* Hadoop automatically detects failure.\n",
    "* It reassigns the failed task to another node (thanks to fault tolerance in HDFS and YARN).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. How is MapReduce better than traditional sequential processing?**\n",
    "\n",
    "* **Parallel execution** increases speed.\n",
    "* **Scalability** – It can run on thousands of machines.\n",
    "* **Fault Tolerance** – If a machine crashes, Hadoop still completes the job.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. What is the command to run a MapReduce job using Hadoop Streaming?**\n",
    "\n",
    "```bash\n",
    "hadoop jar /path/to/hadoop-streaming.jar \\\n",
    "-input /input_dir -output /output_dir \\\n",
    "-mapper mapper.py -reducer reducer.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What are some real-world applications of MapReduce?**\n",
    "\n",
    "* Search engines (like Google Indexing)\n",
    "* Log file analysis\n",
    "* Sentiment analysis\n",
    "* Recommendation systems\n",
    "* Data mining on massive datasets\n",
    "\n",
    "---\n",
    "\n",
    "### **10. What are the limitations of MapReduce?**\n",
    "\n",
    "* Slow for small datasets due to setup overhead.\n",
    "* Lacks built-in iterative support (needed for ML).\n",
    "* Complex to debug.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Summary to Say in Viva:\n",
    "\n",
    "> “In this practical, we designed a distributed application using MapReduce under Hadoop for counting both words and characters in a large text file. We used Python with Hadoop Streaming for flexibility and simplicity. This assignment helped us understand how parallel processing works using Mapper, Shuffle-Sort, and Reducer stages across a Hadoop cluster.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## ✅ **Mapper Code for Character Count**\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3      # Tells the OS to use Python 3 interpreter to run this script\n",
    "\n",
    "import sys                  # Import the sys module to read input from stdin\n",
    "\n",
    "# Loop over each line of input from standard input (provided by Hadoop)\n",
    "for line in sys.stdin:\n",
    "    # Strip any leading/trailing whitespace (like newline characters)\n",
    "    for char in line.strip():\n",
    "        # For each character, print it as a key followed by value 1\n",
    "        # Output format: character<TAB>1\n",
    "        print(f\"{char}\\t1\")\n",
    "```\n",
    "\n",
    "### 🔍 Explanation:\n",
    "\n",
    "1. `#!/usr/bin/env python3` → Makes the script executable in Linux environments using Python 3.\n",
    "2. `import sys` → Allows reading data passed to the script through standard input.\n",
    "3. `for line in sys.stdin:` → Iterates over each line of input coming from Hadoop.\n",
    "4. `line.strip()` → Removes leading and trailing whitespace (including `\\n`).\n",
    "5. `for char in line.strip():` → Iterates character by character in the line.\n",
    "6. `print(f\"{char}\\t1\")` → Emits each character with the value `1`, separated by a tab (`\\t`) for the reducer.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Reducer Code for Character Count**\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3                          # Specifies to use Python 3 interpreter\n",
    "\n",
    "import sys                                      # Required to read from standard input\n",
    "from collections import defaultdict             # Import defaultdict for easy counting\n",
    "\n",
    "counts = defaultdict(int)                       # Create a dictionary to store char counts\n",
    "\n",
    "# Process each line of input from the Mapper\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()                         # Remove newline and extra spaces\n",
    "    if not line:\n",
    "        continue                                # Skip if the line is empty\n",
    "    parts = line.split(\"\\t\")                    # Split the line into key and value\n",
    "    if len(parts) != 2:\n",
    "        continue                                # Skip lines that don’t have exactly 2 parts\n",
    "    key, val = parts                            # Unpack key and value\n",
    "    try:\n",
    "        counts[key] += int(val)                 # Convert val to int and add to count\n",
    "    except ValueError:\n",
    "        continue                                # Skip if val is not an integer\n",
    "\n",
    "# Print sorted output of characters and their total count\n",
    "for key in sorted(counts):                      # Sort keys (characters) alphabetically\n",
    "    print(f\"{key}\\t{counts[key]}\")              # Output: character<TAB>total_count\n",
    "```\n",
    "\n",
    "### 🔍 Explanation:\n",
    "\n",
    "1. `#!/usr/bin/env python3` → Declares Python 3 interpreter.\n",
    "2. `import sys` → Needed for reading input.\n",
    "3. `from collections import defaultdict` → Provides a dictionary with default integer values.\n",
    "4. `counts = defaultdict(int)` → Initializes a dictionary where each key starts at 0.\n",
    "5. `for line in sys.stdin:` → Reads each line from mapper output.\n",
    "6. `line.strip()` → Removes whitespaces and newline characters.\n",
    "7. `if not line: continue` → Skips any empty lines.\n",
    "8. `line.split(\"\\t\")` → Splits the line into a key (character) and a value (1).\n",
    "9. `if len(parts) != 2: continue` → Ensures line has exactly 2 parts.\n",
    "10. `key, val = parts` → Assigns character to `key`, count to `val`.\n",
    "11. `counts[key] += int(val)` → Converts value to int and adds it to the total count.\n",
    "12. `except ValueError: continue` → Ignores lines with invalid numeric values.\n",
    "13. `for key in sorted(counts):` → Sorts characters alphabetically for clean output.\n",
    "14. `print(f\"{key}\\t{counts[key]}\")` → Final output: character with its total count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A) CODE: Word Count \n",
    "1. Open Terminal and switch to Hadoop user \n",
    "pvg@pvg-HP-ProDesk-400-G4-SFF:~$ su hduser \n",
    "Password:  \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:/home/pvg$ cd \n",
    "2. Create a text file to count words \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano word_count.txt \n",
    "#word_count.txt file will open in nano text editor \n",
    "#Add your text in this file \n",
    "#Press Ctrl + X \n",
    "#Press Y \n",
    "#Press Enter key \n",
    "3. Start HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-dfs.sh \n",
    "Starting namenodes on [localhost] \n",
    "Starting datanodes \n",
    "Starting secondary namenodes [pvg-HP-ProDesk-400-G4-SFF] \n",
    "2025-04-21 14:33:31,053 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-yarn.sh \n",
    "Starting resourcemanager \n",
    "Starting nodemanagers \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ jps \n",
    "8275 Jps \n",
    "7894 NodeManager \n",
    "7563 SecondaryNameNode \n",
    "7197 NameNode \n",
    "7325 DataNode \n",
    "7774 ResourceManager \n",
    "4. Create an input directory and upload your file to HDFS: \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls / \n",
    "2025-04-21 14:34:37,504 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 3 items \n",
    "drwxr-xr-x   - hduser supergroup          \n",
    "drwxr-xr-x   \n",
    "drwxr-xr-x   - hduser supergroup           - hduser supergroup       \n",
    "0 2025-04-21 13:53 /input \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -rm -r /input \n",
    "2025-04-21 14:35:47,488 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Deleted /input \n",
    "#Similarly, delete any previous output files if present using: hdfs dfs \n",
    "rm -r /output \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -mkdir -p /input \n",
    "2025-04-21 14:35:55,794 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls / \n",
    "2025-04-21 14:36:06,292 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 3 items \n",
    "drwxr-xr-x   \n",
    "drwxr-xr-x   \n",
    "drwxr-xr-x   - hduser supergroup          - hduser supergroup           - hduser supergroup    \n",
    "0 2025-04-21 14:35 /input \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -put word_count.txt /input/ \n",
    "2025-04-21 14:36:44,153 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /input/ \n",
    "2025-04-21 14:36:55,653 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 1 items -rw-r--r--   1 hduser supergroup         \n",
    "/input/word_count.txt \n",
    "5. Run the word count program: \n",
    "53 2025-04-21 14:36 \n",
    "#we are using the inbuilt WordCount MapReduce program provided in the \n",
    "hadoop-mapreduce-examples.jar file \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ whereis hadoop \n",
    "hadoop: /usr/local/hadoop /usr/local/hadoop/bin/hadoop.cmd \n",
    "/usr/local/hadoop/bin/Hadoop \n",
    "#copy the path /usr/local/Hadoop and rest of the path is in the following \n",
    "command \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hadoop jar \n",
    "/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \n",
    "wordcount /input /output \n",
    "2025-04-21 14:42:53,757 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "2025-04-21 14:42:54,199 INFO impl.MetricsConfig: Loaded properties from hadoop\n",
    "metrics2.properties \n",
    "2025-04-21 14:42:54,270 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot \n",
    "period at 10 second(s). \n",
    "2025-04-21 14:42:54,270 INFO impl.MetricsSystemImpl: JobTracker metrics system \n",
    "started \n",
    "2025-04-21 14:42:54,486 INFO input.FileInputFormat: Total input files to \n",
    "process : 1 \n",
    "2025-04-21 14:42:54,529 INFO mapreduce.JobSubmitter: number of splits:1 \n",
    "2025-04-21 14:42:54,671 INFO mapreduce.JobSubmitter: Submitting tokens for job: \n",
    "job_local1306156686_0001 \n",
    "2025-04-21 14:42:54,671 INFO mapreduce.JobSubmitter: Executing with tokens: [] \n",
    "2025-04-21 14:42:54,785 INFO mapreduce.Job: The url to track the job: \n",
    "http://localhost:8080/ \n",
    "2025-04-21 14:42:54,786 INFO mapreduce.Job: Running job: job_local1306156686_0001 \n",
    "2025-04-21 14:42:54,787 INFO mapred.LocalJobRunner: OutputCommitter set in config \n",
    "null \n",
    "2025-04-21 14:42:54,795 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-21 14:42:54,796 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-21 14:42:54,797 INFO mapred.LocalJobRunner: OutputCommitter is \n",
    "org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter \n",
    "2025-04-21 14:42:54,835 INFO mapred.LocalJobRunner: Waiting for map tasks \n",
    "2025-04-21 14:42:54,836 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local1306156686_0001_m_000000_0 \n",
    "2025-04-21 14:42:54,857 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-21 14:42:54,857 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-21 14:42:54,872 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-21 14:42:54,875 INFO mapred.MapTask: Processing split: \n",
    "hdfs://localhost:54310/input/word_count.txt:0+53 \n",
    "2025-04-21 14:42:54,918 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584) \n",
    "2025-04-21 14:42:54,918 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100 \n",
    "2025-04-21 14:42:54,918 INFO mapred.MapTask: soft limit at 83886080 \n",
    "2025-04-21 14:42:54,918 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600 \n",
    "2025-04-21 14:42:54,918 INFO mapred.MapTask: kvstart = 26214396; length = 6553600 \n",
    "2025-04-21 14:42:54,921 INFO mapred.MapTask: Map output collector class = \n",
    "org.apache.hadoop.mapred.MapTask$MapOutputBuffer \n",
    "2025-04-21 14:42:55,007 INFO mapred.LocalJobRunner:  \n",
    "2025-04-21 14:42:55,008 INFO mapred.MapTask: Starting flush of map output \n",
    "2025-04-21 14:42:55,008 INFO mapred.MapTask: Spilling map output \n",
    "2025-04-21 14:42:55,008 INFO mapred.MapTask: bufstart = 0; bufend = 85; bufvoid = \n",
    "104857600 \n",
    "2025-04-21 14:42:55,008 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend \n",
    "= 26214368(104857472); length = 29/6553600 \n",
    "2025-04-21 14:42:55,023 INFO mapred.MapTask: Finished spill 0 \n",
    "2025-04-21 14:42:55,031 INFO mapred.Task: \n",
    "Task:attempt_local1306156686_0001_m_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-21 14:42:55,035 INFO mapred.LocalJobRunner: map \n",
    "2025-04-21 14:42:55,035 INFO mapred.Task: Task \n",
    "'attempt_local1306156686_0001_m_000000_0' done. \n",
    "2025-04-21 14:42:55,039 INFO mapred.Task: Final Counters for \n",
    "attempt_local1306156686_0001_m_000000_0: Counters: 24 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=281168 \n",
    "  FILE: Number of bytes written=922756 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=53 \n",
    "  HDFS: Number of bytes written=0 \n",
    "  HDFS: Number of read operations=5 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=1 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Map input records=4 \n",
    "  Map output records=8 \n",
    "  Map output bytes=85 \n",
    "  Map output materialized bytes=57 \n",
    "  Input split bytes=108 \n",
    "  Combine input records=8 \n",
    "  Combine output records=4 \n",
    "  Spilled Records=4 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=0 \n",
    "  GC time elapsed (ms)=8 \n",
    "  Total committed heap usage (bytes)=195035136 \n",
    " File Input Format Counters  \n",
    "  Bytes Read=53 \n",
    "2025-04-21 14:42:55,039 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local1306156686_0001_m_000000_0 \n",
    "2025-04-21 14:42:55,039 INFO mapred.LocalJobRunner: map task executor complete. \n",
    "2025-04-21 14:42:55,042 INFO mapred.LocalJobRunner: Waiting for reduce tasks \n",
    "2025-04-21 14:42:55,042 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local1306156686_0001_r_000000_0 \n",
    "2025-04-21 14:42:55,047 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-21 14:42:55,047 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-21 14:42:55,047 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-21 14:42:55,049 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: \n",
    "org.apache.hadoop.mapreduce.task.reduce.Shuffle@cf9471c \n",
    "2025-04-21 14:42:55,050 WARN impl.MetricsSystemImpl: JobTracker metrics system \n",
    "already initialized! \n",
    "2025-04-21 14:42:55,062 INFO reduce.MergeManagerImpl: MergerManager: \n",
    "memoryLimit=1437178240, maxSingleShuffleLimit=359294560, mergeThreshold=948537664, \n",
    "ioSortFactor=10, memToMemMergeOutputsThreshold=10 \n",
    "2025-04-21 14:42:55,063 INFO reduce.EventFetcher: \n",
    "attempt_local1306156686_0001_r_000000_0 Thread started: EventFetcher for fetching \n",
    "Map Completion Events \n",
    "2025-04-21 14:42:55,083 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle \n",
    "output of map attempt_local1306156686_0001_m_000000_0 decomp: 53 len: 57 to \n",
    "MEMORY \n",
    "2025-04-21 14:42:55,085 INFO reduce.InMemoryMapOutput: Read 53 bytes from map\n",
    "output for attempt_local1306156686_0001_m_000000_0 \n",
    "2025-04-21 14:42:55,086 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map\n",
    "output of size: 53, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->53 \n",
    "2025-04-21 14:42:55,087 INFO reduce.EventFetcher: EventFetcher is interrupted.. \n",
    "Returning \n",
    "2025-04-21 14:42:55,088 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-21 14:42:55,088 INFO reduce.MergeManagerImpl: finalMerge called with 1 \n",
    "in-memory map-outputs and 0 on-disk map-outputs \n",
    "2025-04-21 14:42:55,094 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-21 14:42:55,094 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 44 bytes \n",
    "2025-04-21 14:42:55,098 INFO reduce.MergeManagerImpl: Merged 1 segments, 53 bytes \n",
    "to disk to satisfy reduce memory limit \n",
    "2025-04-21 14:42:55,099 INFO reduce.MergeManagerImpl: Merging 1 files, 57 bytes \n",
    "from disk \n",
    "2025-04-21 14:42:55,099 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes \n",
    "from memory into reduce \n",
    "2025-04-21 14:42:55,099 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-21 14:42:55,100 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 44 bytes \n",
    "2025-04-21 14:42:55,100 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-21 14:42:55,116 INFO Configuration.deprecation: mapred.skip.on is \n",
    "deprecated. Instead, use mapreduce.job.skiprecords \n",
    "2025-04-21 14:42:55,172 INFO mapred.Task: \n",
    "Task:attempt_local1306156686_0001_r_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-21 14:42:55,175 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-21 14:42:55,175 INFO mapred.Task: Task \n",
    "attempt_local1306156686_0001_r_000000_0 is allowed to commit now \n",
    "2025-04-21 14:42:55,189 INFO output.FileOutputCommitter: Saved output of task \n",
    "'attempt_local1306156686_0001_r_000000_0' to hdfs://localhost:54310/output \n",
    "2025-04-21 14:42:55,190 INFO mapred.LocalJobRunner: reduce > reduce \n",
    "2025-04-21 14:42:55,190 INFO mapred.Task: Task \n",
    "'attempt_local1306156686_0001_r_000000_0' done. \n",
    "2025-04-21 14:42:55,190 INFO mapred.Task: Final Counters for \n",
    "attempt_local1306156686_0001_r_000000_0: Counters: 30 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=281314 \n",
    "  FILE: Number of bytes written=922813 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=53 \n",
    "  HDFS: Number of bytes written=35 \n",
    "  HDFS: Number of read operations=10 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=3 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Combine input records=0 \n",
    "  Combine output records=0 \n",
    "  Reduce input groups=4 \n",
    "  Reduce shuffle bytes=57 \n",
    "  Reduce input records=4 \n",
    "  Reduce output records=4 \n",
    "  Spilled Records=4 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=0 \n",
    "  Total committed heap usage (bytes)=195035136 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "  IO_ERROR=0 \n",
    "  WRONG_LENGTH=0 \n",
    "  WRONG_MAP=0 \n",
    "  WRONG_REDUCE=0 \n",
    " File Output Format Counters  \n",
    "  Bytes Written=35 \n",
    "2025-04-21 14:42:55,191 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local1306156686_0001_r_000000_0 \n",
    "2025-04-21 14:42:55,191 INFO mapred.LocalJobRunner: reduce task executor complete. \n",
    "2025-04-21 14:42:55,790 INFO mapreduce.Job: Job job_local1306156686_0001 running \n",
    "in uber mode : false \n",
    "2025-04-21 14:42:55,791 INFO mapreduce.Job:  map 100% reduce 100% \n",
    "2025-04-21 14:42:55,794 INFO mapreduce.Job: Job job_local1306156686_0001 \n",
    "completed successfully \n",
    "2025-04-21 14:42:55,800 INFO mapreduce.Job: Counters: 36 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=562482 \n",
    "  FILE: Number of bytes written=1845569 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=106 \n",
    "  HDFS: Number of bytes written=35 \n",
    "  HDFS: Number of read operations=15 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=4 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Map input records=4 \n",
    "  Map output records=8 \n",
    "  Map output bytes=85 \n",
    "  Map output materialized bytes=57 \n",
    "  Input split bytes=108 \n",
    "  Combine input records=8 \n",
    "  Combine output records=4 \n",
    "  Reduce input groups=4 \n",
    "  Reduce shuffle bytes=57 \n",
    "  Reduce input records=4 \n",
    "  Reduce output records=4 \n",
    "  Spilled Records=8 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=8 \n",
    "  Total committed heap usage (bytes)=390070272 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "IO_ERROR=0 \n",
    "WRONG_LENGTH=0 \n",
    "WRONG_MAP=0 \n",
    "WRONG_REDUCE=0 \n",
    "File Input Format Counters  \n",
    "Bytes Read=53 \n",
    "File Output Format Counters  \n",
    "Bytes Written=35 \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls / \n",
    "2025-04-21 14:43:23,963 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 2 items \n",
    "drwxr-xr-x   \n",
    "drwxr-xr-x   - hduser supergroup          - hduser supergroup          \n",
    "6. View Output: \n",
    "0 2025-04-21 14:36 /input \n",
    "0 2025-04-21 14:42 /output \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /output/ \n",
    "2025-04-21 14:43:31,859 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 2 items -rw-r--r--   1 hduser supergroup          -rw-r--r--   1 hduser supergroup         \n",
    "0 2025-04-21 14:42 /output/_SUCCESS \n",
    "35 2025-04-21 14:42 /output/part-r-00000 \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -cat /output/part-r-00000 \n",
    "2025-04-21 14:43:51,634 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hadoop 3 \n",
    "hello \n",
    "2 \n",
    "program 1 \n",
    "world \n",
    "2 \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-dfs.sh  \n",
    "Stopping namenodes on [localhost]  \n",
    "Stopping datanodes  \n",
    "Stopping secondary namenodes [Ubuntu]  \n",
    "2025-04-21 14:44:17,461 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable  \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-yarn.sh  \n",
    "Stopping nodemanagers  \n",
    "Stopping resourcemanager \n",
    "B) CODE: Character Count \n",
    "mapper.py \n",
    "reducer.py \n",
    "1. Open Terminal and switch to Hadoop user \n",
    "pvg@pvg-HP-ProDesk-400-G4-SFF:~$ su hduser                                                 \n",
    "Password:  \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:/home/pvg$ cd \n",
    "2. Start HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-dfs.sh \n",
    "Starting namenodes on [localhost] \n",
    "Starting datanodes \n",
    "Starting secondary namenodes [pvg-HP-ProDesk-400-G4-SFF] \n",
    "2025-04-23 15:07:21,649 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ start-yarn.sh \n",
    "Starting resourcemanager \n",
    "Starting nodemanagers \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ jps \n",
    "12466 NameNode \n",
    "12871 SecondaryNameNode \n",
    "13243 NodeManager \n",
    "12652 DataNode \n",
    "13598 Jps \n",
    "13087 ResourceManager \n",
    "3. Create an input directory \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -mkdir -p /input \n",
    "2025-04-23 15:08:21,712 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls / \n",
    "2025-04-23 15:08:24,979 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 1 items \n",
    "drwxr-xr-x   - hduser supergroup          \n",
    "0 2025-04-23 15:08 /input \n",
    "#Delete any previous input or output files if present using: hdfs dfs -rm -r /input /output \n",
    "4. Create a text file and upload it to HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano character_count.txt \n",
    "#character_count.txt file will open in nano text editor \n",
    "#Add your text in this file \n",
    "#Press Ctrl + X \n",
    "#Press Y \n",
    "#Press Enter key \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -put character_count.txt \n",
    "/input/ \n",
    "2025-04-23 15:09:21,442 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /input/ \n",
    "2025-04-23 15:09:31,281 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 1 items -rw-r--r--   1 hduser supergroup         \n",
    "/input/character_count.txt \n",
    "15 2025-04-23 15:09 \n",
    "5. Similarly, create a mapper.py and reducer.py file \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano mapper.py \n",
    "#This will open the mapper.py file in nano editor; write the Mapper python \n",
    "code here \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ nano reducer.py \n",
    "#Write the Reducer python code \n",
    "#Now, make the python script executable \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ chmod +x mapper.py \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ chmod +x reducer.py \n",
    "6. Run Hadoop streaming jar using the mapper and reducer scripts \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ whereis hadoop \n",
    "hadoop: /usr/local/hadoop /usr/local/hadoop/bin/hadoop.cmd \n",
    "/usr/local/hadoop/bin/Hadoop \n",
    "# - is used to specify flags \n",
    "# \\ at the end of a line is used to split a long command into multiple lines \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hadoop jar \n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\ \n",
    "> -input /input/character_count.txt \\ \n",
    "> -output /output/character_output \\ \n",
    "> -mapper mapper.py \\ \n",
    "> -reducer reducer.py \\ \n",
    "> -file mapper.py \\ \n",
    "> -file reducer.py \n",
    "2025-04-23 15:12:38,863 WARN streaming.StreamJob: -file option is deprecated, \n",
    "please use generic option -files instead. \n",
    "2025-04-23 15:12:38,954 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "packageJobJar: [mapper.py, reducer.py] [] /tmp/streamjob14083921992577679215.jar \n",
    "tmpDir=null \n",
    "2025-04-23 15:12:39,444 INFO impl.MetricsConfig: Loaded properties from hadoop\n",
    "metrics2.properties \n",
    "2025-04-23 15:12:39,514 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot \n",
    "period at 10 second(s). \n",
    "2025-04-23 15:12:39,514 INFO impl.MetricsSystemImpl: JobTracker metrics system \n",
    "started \n",
    "2025-04-23 15:12:39,526 WARN impl.MetricsSystemImpl: JobTracker metrics system \n",
    "already initialized! \n",
    "2025-04-23 15:12:39,710 INFO mapred.FileInputFormat: Total input files to \n",
    "process : 1 \n",
    "2025-04-23 15:12:39,768 INFO mapreduce.JobSubmitter: number of splits:1 \n",
    "2025-04-23 15:12:39,891 INFO mapreduce.JobSubmitter: Submitting tokens for job: \n",
    "job_local333615659_0001 \n",
    "2025-04-23 15:12:39,891 INFO mapreduce.JobSubmitter: Executing with tokens: [] \n",
    "2025-04-23 15:12:40,054 INFO mapred.LocalDistributedCacheManager: Localized \n",
    "file:/home/hduser/mapper.py as \n",
    "file:/app/hadoop/tmp/mapred/local/job_local333615659_0001_c0fdc6e9-bdc9-48dd\n",
    "b275-22476d2aa5ca/mapper.py \n",
    "2025-04-23 15:12:40,074 INFO mapred.LocalDistributedCacheManager: Localized \n",
    "file:/home/hduser/reducer.py as \n",
    "file:/app/hadoop/tmp/mapred/local/job_local333615659_0001_8af362e9-5351-4a5e\n",
    "99a3-0bc92437819d/reducer.py \n",
    "2025-04-23 15:12:40,120 INFO mapreduce.Job: The url to track the job: \n",
    "http://localhost:8080/ \n",
    "2025-04-23 15:12:40,121 INFO mapred.LocalJobRunner: OutputCommitter set in config \n",
    "null \n",
    "2025-04-23 15:12:40,122 INFO mapreduce.Job: Running job: job_local333615659_0001 \n",
    "2025-04-23 15:12:40,126 INFO mapred.LocalJobRunner: OutputCommitter is \n",
    "org.apache.hadoop.mapred.FileOutputCommitter \n",
    "2025-04-23 15:12:40,129 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:12:40,129 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:12:40,170 INFO mapred.LocalJobRunner: Waiting for map tasks \n",
    "2025-04-23 15:12:40,173 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local333615659_0001_m_000000_0 \n",
    "2025-04-23 15:12:40,198 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:12:40,198 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:12:40,207 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-23 15:12:40,216 INFO mapred.MapTask: Processing split: \n",
    "hdfs://localhost:54310/input/character_count.txt:0+15 \n",
    "2025-04-23 15:12:40,230 INFO mapred.MapTask: numReduceTasks: 1 \n",
    "2025-04-23 15:12:40,261 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584) \n",
    "2025-04-23 15:12:40,261 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100 \n",
    "2025-04-23 15:12:40,261 INFO mapred.MapTask: soft limit at 83886080 \n",
    "2025-04-23 15:12:40,261 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600 \n",
    "2025-04-23 15:12:40,261 INFO mapred.MapTask: kvstart = 26214396; length = 6553600 \n",
    "2025-04-23 15:12:40,263 INFO mapred.MapTask: Map output collector class = \n",
    "org.apache.hadoop.mapred.MapTask$MapOutputBuffer \n",
    "2025-04-23 15:12:40,273 INFO streaming.PipeMapRed: PipeMapRed exec \n",
    "[/home/hduser/./mapper.py] \n",
    "2025-04-23 15:12:40,276 INFO Configuration.deprecation: mapred.work.output.dir is \n",
    "deprecated. Instead, use mapreduce.task.output.dir \n",
    "2025-04-23 15:12:40,276 INFO Configuration.deprecation: mapred.local.dir is \n",
    "deprecated. Instead, use mapreduce.cluster.local.dir \n",
    "2025-04-23 15:12:40,277 INFO Configuration.deprecation: map.input.file is \n",
    "deprecated. Instead, use mapreduce.map.input.file \n",
    "2025-04-23 15:12:40,277 INFO Configuration.deprecation: map.input.length is \n",
    "deprecated. Instead, use mapreduce.map.input.length \n",
    "2025-04-23 15:12:40,277 INFO Configuration.deprecation: mapred.job.id is \n",
    "deprecated. Instead, use mapreduce.job.id \n",
    "2025-04-23 15:12:40,277 INFO Configuration.deprecation: mapred.task.partition is \n",
    "deprecated. Instead, use mapreduce.task.partition \n",
    "2025-04-23 15:12:40,278 INFO Configuration.deprecation: map.input.start is \n",
    "deprecated. Instead, use mapreduce.map.input.start \n",
    "2025-04-23 15:12:40,278 INFO Configuration.deprecation: mapred.task.is.map is \n",
    "deprecated. Instead, use mapreduce.task.ismap \n",
    "2025-04-23 15:12:40,278 INFO Configuration.deprecation: mapred.task.id is \n",
    "deprecated. Instead, use mapreduce.task.attempt.id \n",
    "2025-04-23 15:12:40,278 INFO Configuration.deprecation: mapred.tip.id is \n",
    "deprecated. Instead, use mapreduce.task.id \n",
    "2025-04-23 15:12:40,278 INFO Configuration.deprecation: mapred.skip.on is \n",
    "deprecated. Instead, use mapreduce.job.skiprecords \n",
    "2025-04-23 15:12:40,279 INFO Configuration.deprecation: user.name is deprecated. \n",
    "Instead, use mapreduce.job.user.name \n",
    "2025-04-23 15:12:40,354 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:12:40,356 INFO streaming.PipeMapRed: Records R/W=3/1 \n",
    "2025-04-23 15:12:40,357 INFO streaming.PipeMapRed: MRErrorThread done \n",
    "2025-04-23 15:12:40,357 INFO streaming.PipeMapRed: mapRedFinished \n",
    "2025-04-23 15:12:40,359 INFO mapred.LocalJobRunner:  \n",
    "2025-04-23 15:12:40,359 INFO mapred.MapTask: Starting flush of map output \n",
    "2025-04-23 15:12:40,359 INFO mapred.MapTask: Spilling map output \n",
    "2025-04-23 15:12:40,359 INFO mapred.MapTask: bufstart = 0; bufend = 48; bufvoid = \n",
    "104857600 \n",
    "2025-04-23 15:12:40,359 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend \n",
    "= 26214352(104857408); length = 45/6553600 \n",
    "2025-04-23 15:12:40,368 INFO mapred.MapTask: Finished spill 0 \n",
    "2025-04-23 15:12:40,377 INFO mapred.Task: \n",
    "Task:attempt_local333615659_0001_m_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-23 15:12:40,379 INFO mapred.LocalJobRunner: Records R/W=3/1 \n",
    "2025-04-23 15:12:40,379 INFO mapred.Task: Task \n",
    "'attempt_local333615659_0001_m_000000_0' done. \n",
    "2025-04-23 15:12:40,384 INFO mapred.Task: Final Counters for \n",
    "attempt_local333615659_0001_m_000000_0: Counters: 23 \n",
    "File System Counters \n",
    "FILE: Number of bytes read=1047 \n",
    "FILE: Number of bytes written=644740 \n",
    "FILE: Number of read operations=0 \n",
    "FILE: Number of large read operations=0 \n",
    "FILE: Number of write operations=0 \n",
    "HDFS: Number of bytes read=15 \n",
    "HDFS: Number of bytes written=0 \n",
    "HDFS: Number of read operations=5 \n",
    "HDFS: Number of large read operations=0 \n",
    "HDFS: Number of write operations=1 \n",
    "HDFS: Number of bytes read erasure-coded=0 \n",
    "Map-Reduce Framework \n",
    "Map input records=3 \n",
    "Map output records=12 \n",
    "Map output bytes=48 \n",
    "Map output materialized bytes=78 \n",
    "Input split bytes=100 \n",
    "Combine input records=0 \n",
    "Spilled Records=12 \n",
    "Failed Shuffles=0 \n",
    "Merged Map outputs=0 \n",
    "GC time elapsed (ms)=7 \n",
    "Total committed heap usage (bytes)=309329920 \n",
    "File Input Format Counters  \n",
    "Bytes Read=15 \n",
    "2025-04-23 15:12:40,385 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local333615659_0001_m_000000_0 \n",
    "2025-04-23 15:12:40,385 INFO mapred.LocalJobRunner: map task executor complete. \n",
    "2025-04-23 15:12:40,387 INFO mapred.LocalJobRunner: Waiting for reduce tasks \n",
    "2025-04-23 15:12:40,387 INFO mapred.LocalJobRunner: Starting task: \n",
    "attempt_local333615659_0001_r_000000_0 \n",
    "2025-04-23 15:12:40,393 INFO output.FileOutputCommitter: File Output Committer \n",
    "Algorithm version is 2 \n",
    "2025-04-23 15:12:40,393 INFO output.FileOutputCommitter: FileOutputCommitter skip \n",
    "cleanup _temporary folders under output directory:false, ignore cleanup failures: \n",
    "false \n",
    "2025-04-23 15:12:40,393 INFO mapred.Task:  Using ResourceCalculatorProcessTree : \n",
    "[ ] \n",
    "2025-04-23 15:12:40,395 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: \n",
    "org.apache.hadoop.mapreduce.task.reduce.Shuffle@386a06de \n",
    "2025-04-23 15:12:40,396 WARN impl.MetricsSystemImpl: JobTracker metrics system \n",
    "already initialized! \n",
    "2025-04-23 15:12:40,409 INFO reduce.MergeManagerImpl: MergerManager: \n",
    "memoryLimit=1437178240, maxSingleShuffleLimit=359294560, mergeThreshold=948537664, \n",
    "ioSortFactor=10, memToMemMergeOutputsThreshold=10 \n",
    "2025-04-23 15:12:40,410 INFO reduce.EventFetcher: \n",
    "attempt_local333615659_0001_r_000000_0 Thread started: EventFetcher for fetching \n",
    "Map Completion Events \n",
    "2025-04-23 15:12:40,435 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle \n",
    "output of map attempt_local333615659_0001_m_000000_0 decomp: 74 len: 78 to MEMORY \n",
    "2025-04-23 15:12:40,437 INFO reduce.InMemoryMapOutput: Read 74 bytes from map\n",
    "output for attempt_local333615659_0001_m_000000_0 \n",
    "2025-04-23 15:12:40,437 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map\n",
    "output of size: 74, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->74 \n",
    "2025-04-23 15:12:40,439 INFO reduce.EventFetcher: EventFetcher is interrupted.. \n",
    "Returning \n",
    "2025-04-23 15:12:40,439 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:12:40,439 INFO reduce.MergeManagerImpl: finalMerge called with 1 \n",
    "in-memory map-outputs and 0 on-disk map-outputs \n",
    "2025-04-23 15:12:40,445 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-23 15:12:40,445 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 70 bytes \n",
    "2025-04-23 15:12:40,449 INFO reduce.MergeManagerImpl: Merged 1 segments, 74 bytes \n",
    "to disk to satisfy reduce memory limit \n",
    "2025-04-23 15:12:40,449 INFO reduce.MergeManagerImpl: Merging 1 files, 78 bytes \n",
    "from disk \n",
    "2025-04-23 15:12:40,450 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes \n",
    "from memory into reduce \n",
    "2025-04-23 15:12:40,450 INFO mapred.Merger: Merging 1 sorted segments \n",
    "2025-04-23 15:12:40,450 INFO mapred.Merger: Down to the last merge-pass, with 1 \n",
    "segments left of total size: 70 bytes \n",
    "2025-04-23 15:12:40,450 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:12:40,453 INFO streaming.PipeMapRed: PipeMapRed exec \n",
    "[/home/hduser/./reducer.py] \n",
    "2025-04-23 15:12:40,455 INFO Configuration.deprecation: mapred.job.tracker is \n",
    "deprecated. Instead, use mapreduce.jobtracker.address \n",
    "2025-04-23 15:12:40,457 INFO Configuration.deprecation: mapred.map.tasks is \n",
    "deprecated. Instead, use mapreduce.job.maps \n",
    "2025-04-23 15:12:40,489 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:12:40,489 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] \n",
    "out:NA [rec/s] \n",
    "2025-04-23 15:12:40,490 INFO streaming.PipeMapRed: Records R/W=12/1 \n",
    "2025-04-23 15:12:40,492 INFO streaming.PipeMapRed: MRErrorThread done \n",
    "2025-04-23 15:12:40,493 INFO streaming.PipeMapRed: mapRedFinished \n",
    "2025-04-23 15:12:40,544 INFO mapred.Task: \n",
    "Task:attempt_local333615659_0001_r_000000_0 is done. And is in the process of \n",
    "committing \n",
    "2025-04-23 15:12:40,547 INFO mapred.LocalJobRunner: 1 / 1 copied. \n",
    "2025-04-23 15:12:40,547 INFO mapred.Task: Task \n",
    "attempt_local333615659_0001_r_000000_0 is allowed to commit now \n",
    "2025-04-23 15:12:40,564 INFO output.FileOutputCommitter: Saved output of task \n",
    "'attempt_local333615659_0001_r_000000_0' to \n",
    "hdfs://localhost:54310/output/character_output \n",
    "2025-04-23 15:12:40,565 INFO mapred.LocalJobRunner: Records R/W=12/1 > reduce \n",
    "2025-04-23 15:12:40,565 INFO mapred.Task: Task \n",
    "'attempt_local333615659_0001_r_000000_0' done. \n",
    "2025-04-23 15:12:40,566 INFO mapred.Task: Final Counters for \n",
    "attempt_local333615659_0001_r_000000_0: Counters: 30 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=1235 \n",
    "  FILE: Number of bytes written=644818 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=15 \n",
    "  HDFS: Number of bytes written=12 \n",
    "  HDFS: Number of read operations=10 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=3 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Combine input records=0 \n",
    "  Combine output records=0 \n",
    "  Reduce input groups=3 \n",
    "  Reduce shuffle bytes=78 \n",
    "  Reduce input records=12 \n",
    "  Reduce output records=3 \n",
    "  Spilled Records=12 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=0 \n",
    "  Total committed heap usage (bytes)=309329920 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "  IO_ERROR=0 \n",
    "  WRONG_LENGTH=0 \n",
    "  WRONG_MAP=0 \n",
    "  WRONG_REDUCE=0 \n",
    " File Output Format Counters  \n",
    "  Bytes Written=12 \n",
    "2025-04-23 15:12:40,568 INFO mapred.LocalJobRunner: Finishing task: \n",
    "attempt_local333615659_0001_r_000000_0 \n",
    "2025-04-23 15:12:40,569 INFO mapred.LocalJobRunner: reduce task executor complete. \n",
    "2025-04-23 15:12:41,130 INFO mapreduce.Job: Job job_local333615659_0001 running \n",
    "in uber mode : false \n",
    "2025-04-23 15:12:41,131 INFO mapreduce.Job:  map 100% reduce 100% \n",
    "2025-04-23 15:12:41,134 INFO mapreduce.Job: Job job_local333615659_0001 completed \n",
    "successfully \n",
    "2025-04-23 15:12:41,140 INFO mapreduce.Job: Counters: 36 \n",
    " File System Counters \n",
    "  FILE: Number of bytes read=2282 \n",
    "  FILE: Number of bytes written=1289558 \n",
    "  FILE: Number of read operations=0 \n",
    "  FILE: Number of large read operations=0 \n",
    "  FILE: Number of write operations=0 \n",
    "  HDFS: Number of bytes read=30 \n",
    "  HDFS: Number of bytes written=12 \n",
    "  HDFS: Number of read operations=15 \n",
    "  HDFS: Number of large read operations=0 \n",
    "  HDFS: Number of write operations=4 \n",
    "  HDFS: Number of bytes read erasure-coded=0 \n",
    " Map-Reduce Framework \n",
    "  Map input records=3 \n",
    "  Map output records=12 \n",
    "  Map output bytes=48 \n",
    "  Map output materialized bytes=78 \n",
    "  Input split bytes=100 \n",
    "  Combine input records=0 \n",
    "  Combine output records=0 \n",
    "  Reduce input groups=3 \n",
    "  Reduce shuffle bytes=78 \n",
    "  Reduce input records=12 \n",
    "  Reduce output records=3 \n",
    "  Spilled Records=24 \n",
    "  Shuffled Maps =1 \n",
    "  Failed Shuffles=0 \n",
    "  Merged Map outputs=1 \n",
    "  GC time elapsed (ms)=7 \n",
    "  Total committed heap usage (bytes)=618659840 \n",
    " Shuffle Errors \n",
    "  BAD_ID=0 \n",
    "  CONNECTION=0 \n",
    "  IO_ERROR=0 \n",
    "  WRONG_LENGTH=0 \n",
    "  WRONG_MAP=0 \n",
    "  WRONG_REDUCE=0 \n",
    " File Input Format Counters  \n",
    "  Bytes Read=15 \n",
    " File Output Format Counters  \n",
    "Bytes Written=12 \n",
    "2025-04-23 15:12:41,140 INFO streaming.StreamJob: Output directory: \n",
    "/output/character_output \n",
    "7. View Output \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -ls /output/character_output/ \n",
    "2025-04-23 15:13:37,279 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "Found 2 items -rw-r--r--   1 hduser supergroup          \n",
    "/output/character_output/_SUCCESS -rw-r--r--   1 hduser supergroup         \n",
    "/output/character_output/part-00000 \n",
    "0 2025-04-23 15:12 \n",
    "12 2025-04-23 15:12 \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ hdfs dfs -cat \n",
    "/output/character_output/part-00000 \n",
    "2025-04-23 15:14:21,607 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable \n",
    "a 4 \n",
    "b 4 \n",
    "c 4 \n",
    "8. Stop HDFS \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-dfs.sh  \n",
    "Stopping namenodes on [localhost]  \n",
    "Stopping datanodes  \n",
    "Stopping secondary namenodes [Ubuntu]  \n",
    "2025-04-23 15:15:32,461 WARN util.NativeCodeLoader: Unable to load native-hadoop \n",
    "library for your platform... using builtin-java classes where applicable  \n",
    "hduser@pvg-HP-ProDesk-400-G4-SFF:~$ stop-yarn.sh  \n",
    "Stopping nodemanagers  \n",
    "Stopping resourcemanager "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
